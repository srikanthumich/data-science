{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data - Denton, TX\n",
    "\n",
    "The objective of this project is to obtain the open street map of a location - audit, clean the data and import to an SQL database - and perform additional analysis. I chose to analyze Denton city in the state of Texas.\n",
    "\n",
    "I used the Open Street Map Export Tool to find the coordinates of Denton, TX and used Overpass_API link to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 5235,\n",
       " 'meta': 1,\n",
       " 'nd': 293608,\n",
       " 'node': 256686,\n",
       " 'note': 1,\n",
       " 'osm': 1,\n",
       " 'relation': 141,\n",
       " 'tag': 112714,\n",
       " 'way': 23061}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def tags_overview(filename):\n",
    "    '''Parsing through the OSM data file and creating a dictionary - where the \n",
    "    key represents element type and the value represents number of unique \n",
    "    occurances - to understand the overall structure.\n",
    "    '''\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag in tags: \n",
    "            tags[element.tag] += 1\n",
    "        else:\n",
    "            tags[element.tag] = 1\n",
    "    return tags\n",
    "\n",
    "xml_data = os.path.join(\"/Users/Srikanth/Documents/dand/data_wrangling\",\"OSM.xml\") #mapping data directory, file location on system \n",
    "tags_overview(xml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These terms are explained in [Open street map wiki](https://wiki.openstreetmap.org/wiki/Beginners_Guide_1.3). We see that nodes, ways, relations, etc are some of the core elements. For this project I'd explore nodes(dots used to mark locations) and ways (connected line of nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auditing Data\n",
    "\n",
    "Before we process the data and add it into a database, let's check the \"k\" value for each tag and see if there are any potential problems. I used the following regular expressions to filter these tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 50383, 'lower_colon': 60470, 'other': 1861, 'problemchars': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$') #for tags that contain only lowercase letters and are valid\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') #for otherwise valid tags with a colon in their names\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') #for tags with problematic characters\n",
    "\n",
    "def key_type(element, keys):\n",
    "    '''Function that checks with the regular expressions'''\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = process_map(xml_data)\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how many unique users have contributed to this area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        for item in element:\n",
    "            if 'uid' in item.attrib:\n",
    "                users.add(item.attrib['uid'])\n",
    "    return users\n",
    "unique_users = process_map(xml_data)\n",
    "len(unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning Dataset\n",
    "\n",
    "Now let's explore some of the common problems encountered:\n",
    " - Inconsistent street names\n",
    " - Inconsistent postal codes\n",
    "  \n",
    "We'll check the dataset and fix these problems. Starting with street names, we can create a list of expected street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2181': {'FM 2181', 'Farm-to-Market Road 2181'},\n",
       " '288': {'N. Loop 288', 'S Loop 288', 'South Loop 288'},\n",
       " '316': {'Fm 2181 #316'},\n",
       " '35': {'North Interstate 35'},\n",
       " '377': {'South US Highway 377'},\n",
       " '380': {'US 380'},\n",
       " '4201': {'4201'},\n",
       " 'A': {'Avenue A'},\n",
       " 'B': {'South Avenue B'},\n",
       " 'Blvd': {'Hickory Creek Blvd'},\n",
       " 'C': {'Avenue C'},\n",
       " 'Dr': {'Dallas Dr'},\n",
       " 'E': {'S Interstate 35 E'},\n",
       " 'Fwy': {'S stemmons Fwy', 'South Stemmons Fwy'},\n",
       " 'I-35': {'North I-35', 'South I-35'},\n",
       " 'W': {'Ganzar Rd W'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "osm_file = open(xml_data, \"r\")\n",
    "street_types = defaultdict(set)\n",
    "\n",
    "expected = [\"Avenue\", \"Bend\", \"Boulevard\", \"Circle\", \"Commons\", \"Court\", \"Cove\", \"Crossing\", \"Drive\",  \"Hill\",\n",
    "            \"Hollow\", \"Lane\", \"Loop\", \"Park\", \"Parkway\", \"Pass\", \"Path\", \"Place\", \"Plaza\", \"Point\", \"Ridge\", \"Road\", \"Row\",\n",
    "            \"Run\", \"Square\", \"Street\", \"Trail\", \"View\", \"Vista\", \"Way\", \"North\", \"East\", \"West\", \"South\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "           \"street\" :\"Street\",\n",
    "           \"st\": \"Street\",\n",
    "           \"Ave\"  : \"Avenue\",\n",
    "           \"ave\"  : \"Avenue\",\n",
    "           \"Blvd\" : \"Boulevard\",\n",
    "           \"blvd\" : \"Boulevard\",\n",
    "           \"Dr\"   : \"Drive\",\n",
    "           \"dr\"   : \"Drive\",\n",
    "           \"Ln\"   : \"Lane\",\n",
    "           \"ln\"   : \"Lane\",\n",
    "           \"Pkwy\" : \"Parkway\",\n",
    "           \"pkwy\" : \"Parkway\",\n",
    "           \"PKWY\" : \"Parkway\",\n",
    "           \"Rd\"   : \"Road\",\n",
    "           \"Rd.\"  : \"Road\",\n",
    "           \"rd\"   : \"Road\",\n",
    "           \"Ct\"   : \"Court\",\n",
    "           \"ct\"   : \"Court\",\n",
    "           \"Cir\"  : \"Circle\",\n",
    "           \"cir\"  : \"Circle\",\n",
    "           \"Hwy\"  : \"Highway\",\n",
    "           \"HWY\"  : \"Highway\",\n",
    "           \"hwy\"  : \"Highway\",\n",
    "           \"Sq\"   : \"Square\",\n",
    "           \"sq\"   : \"Square\",\n",
    "           \"N\"    : \"North\",\n",
    "           \"N.\"   : \"North\",\n",
    "           \"E\"    : \"East\",\n",
    "           \"W\"    : \"West\",\n",
    "           \"S\"    : \"South\"}\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if (elem.tag == \"node\") or (elem.tag == \"way\"):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "audit(xml_data)\n",
    "dict(street_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4201 => 4201\n",
      "Avenue C => Avenue C\n",
      "Avenue A => Avenue A\n",
      "US 380 => US 380\n",
      "Fm 2181 #316 => Fm 2181 #316\n",
      "S Loop 288 => South Loop 288\n",
      "N. Loop 288 => North Loop 288\n",
      "South Loop 288 => South Loop 288\n",
      "South US Highway 377 => South US Highway 377\n",
      "Ganzar Rd W => Ganzar Road West\n",
      "Dallas Dr => Dallas Drive\n",
      "North Interstate 35 => North Interstate 35\n",
      "Hickory Creek Blvd => Hickory Creek Boulevard\n",
      "FM 2181 => FM 2181\n",
      "Farm-to-Market Road 2181 => Farm-to-Market Road 2181\n",
      "North I-35 => North I-35\n",
      "South I-35 => South I-35\n",
      "South Avenue B => South Avenue B\n",
      "S Interstate 35 E => South Interstate 35 East\n",
      "S stemmons Fwy => South stemmons Fwy\n",
      "South Stemmons Fwy => South Stemmons Fwy\n"
     ]
    }
   ],
   "source": [
    "def update_street_name(name, mapping):\n",
    "    \"\"\" \n",
    "        Replaces unexpected street names with better names \n",
    "\n",
    "        Args:\n",
    "            name: An unexpected street name\n",
    "            mapping: Dictionary of expected street names\n",
    "\n",
    "        Returns:\n",
    "            name: The updated street name\n",
    "    \"\"\"\n",
    "    words = name.split()\n",
    "    for w in range(len(words)):\n",
    "        if words[w] in mapping:\n",
    "            words[w] = mapping[words[w]]\n",
    "    name = \" \".join(words)\n",
    "    return name\n",
    "\n",
    "for street_type, ways in street_types.items():\n",
    "    for name in ways:\n",
    "        better_name = update_street_name(name, mapping)\n",
    "        print (name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'75': {'75065', '75077', '75201'},\n",
       " '76': {'76201',\n",
       "  '76203',\n",
       "  '76205',\n",
       "  '76207',\n",
       "  '76208',\n",
       "  '76209',\n",
       "  '76209-1540',\n",
       "  '76210',\n",
       "  '76226',\n",
       "  '76227',\n",
       "  '76249',\n",
       "  '76259'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def audit_zipcode(invalid_zipcodes, zipcodes):\n",
    "    if not (re.match(r'^()\\d{3}$', zipcodes)): \n",
    "        invalid_zipcodes[zipcodes[:2]].add(zipcodes)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    invalid_zipcodes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes,tag.attrib['v'])\n",
    "\n",
    "    return invalid_zipcodes\n",
    "\n",
    "cal_zipcode = audit_zip(xml_data)\n",
    "dict(cal_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76205 => ['76205']\n",
      "76259 => ['76259']\n",
      "76249 => ['76249']\n",
      "76207 => ['76207']\n",
      "76208 => ['76208']\n",
      "76227 => ['76227']\n",
      "76201 => ['76201']\n",
      "76226 => ['76226']\n",
      "76209 => ['76209']\n",
      "76209-1540 => 76209\n",
      "76203 => ['76203']\n",
      "76210 => ['76210']\n",
      "75077 => ['75077']\n",
      "75065 => ['75065']\n",
      "75201 => ['75201']\n"
     ]
    }
   ],
   "source": [
    "def update_zipcode(zipcode):\n",
    "    test = re.findall('[a-zA-z]*', zipcode)\n",
    "    if test:\n",
    "        better_zipcode = re.findall(r'\\d+', zipcode)\n",
    "        if better_zipcode:\n",
    "            if len(better_zipcode) == 2:\n",
    "                return better_zipcode[0]\n",
    "            else:\n",
    "                return better_zipcode\n",
    "\n",
    "for zipcode, ways in cal_zipcode.items():\n",
    "    for name in ways:\n",
    "        better_name = update_zipcode(name)\n",
    "        print (name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the auditing is complete the next step is to prepare the data to be inserted into a SQL database. To do so we need to parse the elements in the OSM XML file, transforming them from document format to tabular format, thus making it possible to write to .csv files. These csv files can then easily be imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    " - Use iterparse to iteratively step through each top level element in the XML\n",
    " - Shape each element into several data structures using a custom function\n",
    " - Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    " - Write each data structure to the appropriate .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cerberus in /Users/Srikanth/anaconda/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install cerberus\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "import unicodecsv\n",
    "\n",
    "\n",
    "OSM_PATH = \"OSM.xml\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def shape_element(element, node_attr_fields = NODE_FIELDS, way_attr_fields = WAY_FIELDS,\n",
    "                  problem_chars = PROBLEMCHARS, default_tag_type = 'regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for node in NODE_FIELDS:\n",
    "            node_attribs[node] = element.attrib[node]\n",
    "        for child in element:\n",
    "            tag = {}\n",
    "            if PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "                continue\n",
    "        \n",
    "            elif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "                tag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "                tag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "                tag[\"key\"] = tag_key\n",
    "                if tag_type:\n",
    "                    tag[\"type\"] = tag_type\n",
    "                else:\n",
    "                    tag[\"type\"] = 'regular'\n",
    "            \n",
    "                tag[\"id\"] = element.attrib[\"id\"]\n",
    "                tag[\"value\"] = child.attrib[\"v\"]\n",
    "            else:\n",
    "                tag[\"value\"] = child.attrib[\"v\"]\n",
    "                tag[\"key\"] = child.attrib[\"k\"]\n",
    "                tag[\"type\"] = \"regular\"\n",
    "                tag[\"id\"] = element.attrib[\"id\"]\n",
    "            if tag:\n",
    "                tags.append(tag)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "        for child in element:\n",
    "            nd = {}\n",
    "            tag = {}\n",
    "            if child.tag == 'tag':\n",
    "                if PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "                    continue\n",
    "                elif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "                    tag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "                    tag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "                    tag[\"key\"] = tag_key\n",
    "                    if tag_type:\n",
    "                        tag[\"type\"] = tag_type\n",
    "                    else:\n",
    "                        tag[\"type\"] = 'regular'\n",
    "                    tag[\"id\"] = element.attrib[\"id\"]\n",
    "                    tag[\"value\"] = child.attrib[\"v\"]\n",
    "    \n",
    "                else:\n",
    "                    tag[\"value\"] = child.attrib[\"v\"]\n",
    "                    tag[\"key\"] = child.attrib[\"k\"]\n",
    "                    tag[\"type\"] = \"regular\"\n",
    "                    tag[\"id\"] = element.attrib[\"id\"]\n",
    "                if tag:\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                nd['id'] = element.attrib[\"id\"]\n",
    "                nd['node_id'] = child.attrib[\"ref\"]\n",
    "                nd['position'] = len(way_nodes)\n",
    "            \n",
    "                if nd:\n",
    "                    way_nodes.append(nd)\n",
    "            else:\n",
    "                continue\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.items())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.items()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new db\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db = sqlite3.connect(\"/Users/Srikanth/Documents/dand/data_wrangling/OpenStreetMap_Denton.db\")\n",
    "db.text_factory = str\n",
    "c = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_files = ['nodes', 'nodes_tags', 'ways', 'ways_tags', 'ways_nodes']\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file + '.csv')\n",
    "    df.to_sql(file, db, if_exists='append', index=False)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Nodes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>256686</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(256686,)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install ipython-sql\n",
    "# !pip install pymysql\n",
    "# %reload_ext sql\n",
    "%sql sqlite:///OpenStreetMap_Denton.db\n",
    "%config SqlMagic.feedback = False\n",
    "\n",
    "sqlite_file = '/Users/Srikanth/Documents/dand/data_wrangling/OpenStreetMap_Denton.db'\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "%sql SELECT COUNT(*) as Nodes FROM nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Ways</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>23061</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(23061,)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*) as Ways FROM ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sqlite3.OperationalError) no such column: uid [SQL: \"SELECT COUNT(DISTINCT(user.uid)) as 'Number of unique users' FROM           (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) as user\"]\n"
     ]
    }
   ],
   "source": [
    "%sql SELECT COUNT(DISTINCT(user.uid)) as 'Number of unique users' FROM \\\n",
    "          (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) as user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Number of unique users</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1,)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(DISTINCT('user.uid')) as 'Number of unique users' FROM \\\n",
    "          (SELECT 'uid' FROM nodes UNION ALL SELECT 'uid' FROM ways) as user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
